{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIC-SURE python API use-case: Phenome-Wide analysis on BioData Catalyst studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an illustration example about how to query data using the python **PIC-SURE API**. It takes as use-case a simple PheWAS analysis. This notebook is intentionally straightforward, and explanation provided are only aimed at guiding through the PheWAS analysis process. For a more step-by-step introduction to the python PIC-SURE API, see the `1_PICSURE_API_101.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this notebook, please be sure to get a user-specific security token. For more information on how to proceed, see the `get_your_token.ipynb` notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System requirements\n",
    "- Python 3.6 or later\n",
    "- pip package manager\n",
    "- bash interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of external dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing latest python PIC-SURE API libraries from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-client.git\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-biodatacatalyst-python-adapter-hpds.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import PicSureClient\n",
    "import PicSureBdcAdapter\n",
    "\n",
    "from python_lib.utils import get_multiIndex_variablesDict, joining_variablesDict_onCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"NB: This Jupyter Notebook has been written using PIC-SURE API following versions:\\n- PicSureBdcAdapter: 1.0.0\\n- PicSureClient: 1.1.0\")\n",
    "print(\"The installed PIC-SURE API libraries versions:\\n- PicSureBdcAdapter: {0}\\n- PicSureClient: {1}\".format(PicSureBdcAdapter.__version__, PicSureClient.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Pandas DataFrame display options\n",
    "pd.set_option(\"max.rows\", 100)\n",
    "\n",
    "# Matplotlib display parameters\n",
    "plt.rcParams[\"figure.figsize\"] = (14,8)\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to a PIC-SURE network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "PICSURE_network_URL = \"https://picsure.biodatacatalyst.nhlbi.nih.gov/picsure\"\n",
    "resource_id = \"02e23f52-f354-4e8b-992c-d37c8b9ba140\"\n",
    "token_file = \"token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open(token_file, \"r\") as f:\n",
    "    my_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "client = PicSureClient.Client()\n",
    "connection = client.connect(PICSURE_network_URL, my_token)\n",
    "adapter = PicSureBdcAdapter.Adapter(connection)\n",
    "resource = adapter.useResource(resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PheWAS analysis\n",
    "*Note: This example is not meant to be publication-ready, but rather serve as a guide or starting point to perform PheWAS.*\n",
    "\n",
    "This PheWAS analysis focuses on the TopMed DCC Harmonized Variables. \n",
    "We leverage the harmonized variables to provide an example PheWAS focused on total cholesterol in two studies: ARIC and FHS.\n",
    "The PIC-SURE API is helpful in wrangling our phenotypic data. \n",
    "\n",
    "In a nutshell, this PheWAS analysis follows the subsequent steps:\n",
    "1. Retrieving the variable dictionary, using the PIC-SURE API dedicated methods\n",
    "2. Using the PIC-SURE API to select variables and retrieve data\n",
    "3. Data management\n",
    "4. Statistical analysis for each study and sex\n",
    "5. Visualization of results in Manhattan Plot\n",
    "\n",
    "With this, we are tackling two different analysis considerations of a PheWAS: \n",
    "1. Using multiple variables in a PheWAS. In this example, we are looking into sex differences of total \n",
    "2. Harmonization and meta-analysis issues when using data from multiple studies or datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieving variable dictionary from PIC-SURE\n",
    "The first step to conducting the PheWAS is to retrieve information about the variables that will be used in the analysis. For this example, we will be using variables from the TOPMed Data Coordinating Center (DCC) Harmonized data set. \n",
    "\n",
    "The Data Harmonization effort aims to produce \"a high quality, lasting resource of publicly available and thoroughly documented harmonized phenotype variables\". The TOPMed DCC collaborates with Working Group members and phenotype experts on this endeavour. So far, 44 harmonized variables are accessible through PIC-SURE API (in addition to the age at which each variable value has been collected for a given subject).\n",
    "\n",
    "Which phenotypic characteristics are included in the harmonized variables?\n",
    "\n",
    "- Key NHLBI phenotypes\n",
    "    - Blood cell counts\n",
    "    - VTE\n",
    "    - Atherosclerosis-related phenotypes\n",
    "    - Lipids\n",
    "    - Blood pressure\n",
    "    \n",
    "    \n",
    "- Common covariates\n",
    "    - Height\n",
    "    - Weight\n",
    "    - BMI\n",
    "    - Smoking status\n",
    "    - Race/ethnicity\n",
    "\n",
    "More information about the variable harmonization process is available at https://www.nhlbiwgs.org/sites/default/files/pheno_harmonization_guidelines.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we retrieve the harmonized variables information by searching for `DCC Harmonized data set` in PIC-SURE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "harmonized_variables = resource.dictionary().find(\"DCC Harmonized data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_dic = harmonized_variables.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Display the variables tree hierarchy from the variables name\n",
    "variablesDict = get_multiIndex_variablesDict(harmonized_dic)\n",
    "variablesDict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we retrieved 80 variables from the DCC harmonized data set. The structure of `variablesDict` allows us to visualize the tree-like structure of the concept paths more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using the PIC-SURE API to select variables and retrieve data\n",
    "Now that we've retrieved the variable information, we need to select our variable of interest. In this example, we are interested in exploring the relationship between the harmonized variables and blood cholesterol. Specifically, we will find the full concept path that contains \"Blood mass concentration of total cholesterol\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_harmonized_dic = harmonized_variables.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dependent variable - total cholesterol\n",
    "cholesterol_path = my_harmonized_dic.filter(like = 'Blood mass concentration of total cholesterol', axis = 0)\n",
    "cholesterol_path = list(cholesterol_path.index)[0]\n",
    "cholesterol_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full list of concept paths with cholesterol_path removed\n",
    "selected_vars = list(variablesDict['name'])\n",
    "selected_vars.remove(cholesterol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create our query and retrieve the dataframe. This query will consist of two parts:\n",
    "1. **Any record of `cholesterol_path`.** By performing an \"any record of\" filter on the `cholesterol_path`, we will filter out all participants that do not have total blood cholesterol measurements. This allows us to perform more meaningful statistical analysis on the data.\n",
    "2. **Select all remaining harmonized variables.** We will then add all of the remaining harmonized variables to the query, which will allow us to retrieve this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a query\n",
    "query = resource.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.anyof().add(cholesterol_path) # Use anyof for the cholesterol variable to filter out the NA values\n",
    "query.select().add(selected_vars)\n",
    "facts = query.getResultsDataFrame(low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data-management\n",
    "Now that we have retrieved the data, we shall perform some data management steps to prepare for the statistical analysis. First, we will identify which variables are categorical and which are continuous using the \"categorical\" column of the `facts` dataframe. This is an example of how the PIC-SURE API greatly simplifies this step for the user, as categorizing variables can be tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_varnames = my_harmonized_dic[my_harmonized_dic.categorical == True]\n",
    "categorical_varnames = list(categorical_varnames.index)\n",
    "\n",
    "continuous_varnames = my_harmonized_dic[my_harmonized_dic.categorical == False]\n",
    "continuous_varnames = list(continuous_varnames.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cholesterol_path from continuous_varnames\n",
    "continuous_varnames.remove(cholesterol_path) \n",
    "# remove subgroup concept path from categorical_varnames\n",
    "categorical_varnames.remove(\"\\\\DCC Harmonized data set\\\\01 - Demographics\\\\A distinct subgroup within a study  generally indicating subjects who share similar characteristics due to study design. Subjects may belong to only one subcohort.\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "To perform this PheWAS, we will frame two participant cohorts in the context of the dependent variable of interest. In this example, we are interested in blood cholesterol. However, `Blood mass concentation of total cholesterol` is a continuous variable. We shall convert this variable into a binary variable with two groups, Normal/Low and High cholesterol levels, by applying a [threshold of 200mg/dL](https://www.mayoclinic.org/diseases-conditions/high-blood-cholesterol/diagnosis-treatment/drc-20350806). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    list(facts[cholesterol_path] <= 200),\n",
    "    list(facts[cholesterol_path] > 200)\n",
    "]\n",
    "outputs = ['Normal/Low', 'High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.select(conditions, outputs)\n",
    "facts['categorical_cholesterol'] = pd.Series(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also specify the variable name for the covariate we are interested in, in this case Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_path = list(facts.filter(regex = 'sex'))[0]\n",
    "sex_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also select our cohorts of interest. In this example, we are interested in participants from the Framingham Heart Study (FHS) and the Atherosclerosis Risk In Communities (ARIC) cohort. We can utilize the `A distinct subgroup within a study  generally indicating subjects who share similar characteristics due to study design. Subjects may belong to only one subcohort.` concept path in the DCC Harmonized data set to select the participants of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_varnames = my_harmonized_dic[my_harmonized_dic.categorical == True]\n",
    "categorical_varnames = list(categorical_varnames.index)\n",
    "\n",
    "continuous_varnames = my_harmonized_dic[my_harmonized_dic.categorical == False]\n",
    "continuous_varnames = list(continuous_varnames.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts['FHS_cohort'] = facts.iloc[:,1].str.contains('FHS')\n",
    "facts['ARIC_cohort'] = facts.iloc[:,1].str.contains('ARIC')\n",
    "\n",
    "fhs_subset = facts[facts.FHS_cohort == True]\n",
    "aric_subset = facts[facts.ARIC_cohort == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Univariate statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, each variable present in the `facts_dummies` dataset will be tested again the selected dependent variable, (ie presence or absence of COPD). \n",
    "\n",
    "Two different association tests will be carried out according to variables data types: \n",
    "- Mann-Whitney U test for continuous ones\n",
    "- Fisher exact test for categorical ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLR result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "independent_names = variablesDict[\"name\"].tolist()\n",
    "independent_names.remove(dependent_var_name)\n",
    "dependent_var = facts[dependent_var_name].astype(\"category\").cat.codes\n",
    "dic_pvalues = {}\n",
    "simple_index_variablesDict = variablesDict.set_index(\"name\", drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import LinAlgError\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for independent_name in tqdm(independent_names, position=0, leave=True):\n",
    "    matrix = facts.loc[:, [dependent_var_name, independent_name]]\\\n",
    "                  .dropna(how=\"any\")\n",
    "    if matrix.shape[0] == 0:\n",
    "        dic_pvalues[independent_name] = np.NaN\n",
    "        continue\n",
    "    if simple_index_variablesDict.loc[independent_name, \"categorical\"]:\n",
    "        matrix = pd.get_dummies(matrix,\n",
    "                                columns=[independent_name],\n",
    "                                drop_first=False)\\\n",
    "                    .iloc[:, 0:-1]\n",
    "    dependent_var = matrix[dependent_var_name].cat.codes\n",
    "    independent_var = matrix.drop(dependent_var_name, axis=1)\\\n",
    "                            .assign(intercept = 1)\n",
    "    model = Logit(dependent_var, independent_var)\n",
    "    try:\n",
    "        results = model.fit(disp=0)\n",
    "        dic_pvalues[independent_name] = results.llr_pvalue\n",
    "    except (LinAlgError, PerfectSeparationError) as e:\n",
    "        dic_pvalues[independent_name] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p-values distribution (univariate tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "pd.Series([v for v in dic_pvalues.values()]).plot.hist(bins=30)\n",
    "plt.suptitle(\"Distribution of individual p-values\",\n",
    "             weight=\"bold\",\n",
    "            fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multiple hypotheses testing correction: Bonferroni Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle the multiple testing problem (increase in the probability of getting statistically significant associations), we will use the Bonferroni correction method. Although many other multiple-comparison corrections exist, Bonferroni is the most straightforward to use, because it doesn't require assumptions about variables correlation. Other PheWAS analysis also use False Discovery Rate controlling procedures ([see](https://en.wikipedia.org/wiki/False_discovery_rate))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, Bonferonni allows to calculate a corrected \"statistical significant threshold\" according to the number of test performed. Every p-value below this threshold will be deemed statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Merging pvalues from different tests\n",
    "df_pvalues = pd.DataFrame.from_dict(dic_pvalues, orient=\"index\", columns=[\"pvalues\"])\\\n",
    ".rename_axis(\"name\")\\\n",
    ".reset_index(drop=False)\n",
    "\n",
    "# Adding pvalues results as a new column to variablesDict\n",
    "variablesDict = joining_variablesDict_onCol(variablesDict,\n",
    "                                              df_pvalues,\n",
    "                                              left_col=\"name\",\n",
    "                                              right_col=\"name\")\n",
    "\n",
    "adjusted_alpha = 0.05/len(variablesDict[\"pvalues\"])\n",
    "variablesDict[\"p_adj\"] = variablesDict[\"pvalues\"] / len(variablesDict[\"pvalues\"])\n",
    "variablesDict['log_p'] = -np.log10(variablesDict['pvalues'])\n",
    "variablesDict = variablesDict.sort_index()\n",
    "variablesDict[\"group\"] = variablesDict.reset_index(level=2)[\"level_2\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Bonferonni adjusted significance threshold: {0:.2E}\".format(adjusted_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Result visualisations: Manhattan plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manhattan plot is the classical way to plot the results of a PheWAS analysis. It plots every tested phenotypical variables on the X-axis, against their *-log(pvalue)* on the Y-axis. The horizontal line represents the adjusted significance level threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mask = variablesDict[\"pvalues\"].isna()\n",
    "df_results = variablesDict.loc[~mask,:].copy().replace([np.inf, -np.inf], np.nan)\n",
    "df_results = df_results.loc[~df_results[\"log_p\"].isna().values,:]\n",
    "\n",
    "#### Specific adjustment to make this specific plot looks nicer\n",
    "####### to adapt when changing data or dependent variable\n",
    "df_results = df_results.replace({\"TLC\": \"Spirometry\",\n",
    "                                 \"New Gold Classification\": \"Quantitative Analysis\", \n",
    "                  \"Other\": \"Demographics\",\n",
    "                                \"Eligibility Form\": \"Sociodemography and Administration\"})\n",
    "group_order={'6MinWalk': 0,\n",
    " 'CT Acquisition Parameters': 1,\n",
    " 'CT Assessment Scoresheet': 2,\n",
    " 'Demographics and Physical Characteristics': 3,\n",
    " 'Longitudinal Analysis': 5,\n",
    " 'Medical History': 4,\n",
    " 'Medication History': 13,\n",
    " 'Quantitative Analysis': 9,\n",
    " 'Respiratory Disease': 6,\n",
    " 'SF-36 Health Survey': 11,\n",
    " 'Sociodemography and Administration': 12,\n",
    " 'Spirometry': 7,\n",
    " 'VIDA': 15}\n",
    "df_results[\"group_order\"] = df_results[\"group\"].replace(group_order)\n",
    "df_results = df_results.sort_values(\"group_order\", ascending=True)\n",
    "df_results[\"simplified_name\"] = df_results[\"simplified_name\"].str.replace(\"[0-9]+[A-z]*\", \"\").to_frame()\n",
    "###\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "colors = plt.get_cmap('Set1')\n",
    "x_labels = []\n",
    "x_labels_pos = []\n",
    "\n",
    "y_lims = (0, df_results[\"log_p\"].max(skipna=True) + 50)\n",
    "threshold_top_values = df_results[\"log_p\"].sort_values(ascending=False)[0:6][-1]\n",
    "\n",
    "df_results[\"ind\"] = np.arange(1, len(df_results)+1)\n",
    "df_grouped = df_results.groupby(('group'))\n",
    "for num, (name, group) in enumerate(df_grouped):\n",
    "    group.plot(kind='scatter', x='ind', y='log_p',color=colors.colors[num % len(colors.colors)], ax=ax, s=20)\n",
    "    x_labels.append(name)\n",
    "    x_labels_pos.append((group['ind'].iloc[-1] - (group['ind'].iloc[-1] - group['ind'].iloc[0])/2)) # Set label in the middle\n",
    "    for n, row in group.iterrows():\n",
    "        if row[\"log_p\"] > threshold_top_values:\n",
    "            ax.text(row['ind'] + 3, row[\"log_p\"] + 0.05, row[\"simplified_name\"], rotation=0, alpha=1, size=8, color=\"black\")\n",
    "                \n",
    "ax.set_xticks(x_labels_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_xlim([0, len(df_results) +1])\n",
    "ax.set_ylim(y_lims)\n",
    "ax.set_ylabel('-log(p-values)', style=\"italic\")\n",
    "ax.set_xlabel('Phenotypes', fontsize=15)\n",
    "ax.axhline(y=-np.log10(adjusted_alpha), linestyle=\":\", color=\"black\", label=\"Adjusted Threshold\")\n",
    "plt.xticks(fontsize = 9,rotation=90)\n",
    "plt.yticks(fontsize = 8)\n",
    "plt.title(\"Statistical Association Between COPD Status and Phenotypes\", \n",
    "          loc=\"center\",\n",
    "          style=\"oblique\", \n",
    "          fontsize = 20,\n",
    "         y=1)\n",
    "xticks = ax.xaxis.get_major_ticks()\n",
    "xticks[0].set_visible(False)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "plt.legend(handles = handles, labels = labels, loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it appears that most of the tested phenotypes covariates are above the adjusted threshold of significant association. However, it is not that surprising given the nature of our dependent variable: a lot of the collected phenotypic variables are correlated to the COPD status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be used directly with any other variable present in the variable Dictionary. Only the `dependent_var_name` variable need to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
