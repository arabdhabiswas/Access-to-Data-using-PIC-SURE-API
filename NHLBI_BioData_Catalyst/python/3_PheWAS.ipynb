{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIC-SURE python API use-case: Phenome-Wide analysis on BioData Catalyst studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an illustration example about how to query data using the python **PIC-SURE API**. It takes as use-case a simple PheWAS analysis. This notebook is intentionally straightforward, and explanation provided are only aimed at guiding through the PheWAS analysis process. For a more step-by-step introduction to the python PIC-SURE API, see the `1_PICSURE_API_101.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this notebook, please be sure to get a user-specific security token. For more information on how to proceed, see the `get_your_token.ipynb` notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System requirements\n",
    "- Python 3.6 or later\n",
    "- pip package manager\n",
    "- bash interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of external dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing latest python PIC-SURE API libraries from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-client.git\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-biodatacatalyst-python-adapter-hpds.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import PicSureClient\n",
    "import PicSureBdcAdapter\n",
    "\n",
    "from python_lib.utils import get_multiIndex_variablesDict, joining_variablesDict_onCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"NB: This Jupyter Notebook has been written using PIC-SURE API following versions:\\n- PicSureBdcAdapter: 1.0.0\\n- PicSureClient: 1.1.0\")\n",
    "print(\"The installed PIC-SURE API libraries versions:\\n- PicSureBdcAdapter: {0}\\n- PicSureClient: {1}\".format(PicSureBdcAdapter.__version__, PicSureClient.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Pandas DataFrame display options\n",
    "pd.set_option(\"max.rows\", 100)\n",
    "\n",
    "# Matplotlib display parameters\n",
    "plt.rcParams[\"figure.figsize\"] = (14,8)\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to a PIC-SURE network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "PICSURE_network_URL = \"https://picsure.biodatacatalyst.nhlbi.nih.gov/picsure\"\n",
    "resource_id = \"02e23f52-f354-4e8b-992c-d37c8b9ba140\"\n",
    "token_file = \"token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open(token_file, \"r\") as f:\n",
    "    my_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "client = PicSureClient.Client()\n",
    "connection = client.connect(PICSURE_network_URL, my_token)\n",
    "adapter = PicSureBdcAdapter.Adapter(connection)\n",
    "resource = adapter.useResource(resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PheWAS analysis\n",
    "*Note: This example is not meant to be publication-ready, but rather serve as a guide or starting point to perform PheWAS.*\n",
    "\n",
    "This PheWAS analysis focuses on the TopMed DCC Harmonized Variables. \n",
    "We leverage the harmonized variables to provide an example PheWAS focused on total cholesterol in two studies: ARIC and FHS.\n",
    "The PIC-SURE API is helpful in wrangling our phenotypic data. \n",
    "\n",
    "In a nutshell, this PheWAS analysis follows the subsequent steps:\n",
    "1. Retrieving the variable dictionary, using the PIC-SURE API dedicated methods\n",
    "2. Using the PIC-SURE API to select variables and retrieve data\n",
    "3. Data management\n",
    "4. Statistical analysis for each study and sex\n",
    "5. Visualization of results in Manhattan Plot\n",
    "\n",
    "With this, we are tackling two different analysis considerations of a PheWAS: \n",
    "1. Using multiple variables in a PheWAS. In this example, we are looking into sex differences of total \n",
    "2. Harmonization and meta-analysis issues when using data from multiple studies or datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retrieving variable dictionary from PIC-SURE\n",
    "The first step to conducting the PheWAS is to retrieve information about the variables that will be used in the analysis. For this example, we will be using variables from the TOPMed Data Coordinating Center (DCC) Harmonized data set. \n",
    "\n",
    "The Data Harmonization effort aims to produce \"a high quality, lasting resource of publicly available and thoroughly documented harmonized phenotype variables\". The TOPMed DCC collaborates with Working Group members and phenotype experts on this endeavour. So far, 44 harmonized variables are accessible through PIC-SURE API (in addition to the age at which each variable value has been collected for a given subject).\n",
    "\n",
    "Which phenotypic characteristics are included in the harmonized variables?\n",
    "\n",
    "- Key NHLBI phenotypes\n",
    "    - Blood cell counts\n",
    "    - VTE\n",
    "    - Atherosclerosis-related phenotypes\n",
    "    - Lipids\n",
    "    - Blood pressure\n",
    "    \n",
    "    \n",
    "- Common covariates\n",
    "    - Height\n",
    "    - Weight\n",
    "    - BMI\n",
    "    - Smoking status\n",
    "    - Race/ethnicity\n",
    "\n",
    "More information about the variable harmonization process is available at https://www.nhlbiwgs.org/sites/default/files/pheno_harmonization_guidelines.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we retrieve the harmonized variables information by searching for `DCC Harmonized data set` in PIC-SURE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "harmonized_variables = resource.dictionary().find(\"DCC Harmonized data set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_dic = harmonized_variables.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Display the variables tree hierarchy from the variables name\n",
    "variablesDict = get_multiIndex_variablesDict(harmonized_dic)\n",
    "variablesDict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we retrieved 80 variables from the DCC harmonized data set. The structure of `variablesDict` allows us to visualize the tree-like structure of the concept paths more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using the PIC-SURE API to select variables and retrieve data\n",
    "Now that we've retrieved the variable information, we need to select our variable of interest. In this example, we are interested in exploring the relationship between the harmonized variables and blood cholesterol. Specifically, we will find the full concept path that contains \"Blood mass concentration of total cholesterol\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_harmonized_dic = harmonized_variables.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the dependent variable - total cholesterol\n",
    "cholesterol_path = my_harmonized_dic.filter(like = 'Blood mass concentration of total cholesterol', axis = 0)\n",
    "cholesterol_path = list(cholesterol_path.index)[0]\n",
    "cholesterol_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full list of concept paths with cholesterol_path removed\n",
    "selected_vars = list(variablesDict['name'])\n",
    "selected_vars.remove(cholesterol_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create our query and retrieve the dataframe. This query will consist of two parts:\n",
    "1. **Any record of `cholesterol_path`.** By performing an \"any record of\" filter on the `cholesterol_path`, we will filter out all participants that do not have total blood cholesterol measurements. This allows us to perform more meaningful statistical analysis on the data.\n",
    "2. **Select all remaining harmonized variables.** We will then add all of the remaining harmonized variables to the query, which will allow us to retrieve this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a query\n",
    "query = resource.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.anyof().add(cholesterol_path) # Use anyof for the cholesterol variable to filter out the NA values\n",
    "query.select().add(selected_vars)\n",
    "facts = query.getResultsDataFrame(low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data-management\n",
    "Now that we have retrieved the data, we shall perform some data management steps to prepare for the statistical analysis. First, we will identify which variables are categorical and which are continuous using the \"categorical\" column of the `facts` dataframe. This is an example of how the PIC-SURE API greatly simplifies this step for the user, as categorizing variables can be tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_varnames = my_harmonized_dic[my_harmonized_dic.categorical == True]\n",
    "categorical_varnames = list(categorical_varnames.index)\n",
    "\n",
    "continuous_varnames = my_harmonized_dic[my_harmonized_dic.categorical == False]\n",
    "continuous_varnames = list(continuous_varnames.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cholesterol_path from continuous_varnames\n",
    "continuous_varnames.remove(cholesterol_path) \n",
    "# remove subgroup concept path from categorical_varnames\n",
    "categorical_varnames.remove(\"\\\\DCC Harmonized data set\\\\01 - Demographics\\\\A distinct subgroup within a study  generally indicating subjects who share similar characteristics due to study design. Subjects may belong to only one subcohort.\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "To perform this PheWAS, we will frame two participant cohorts in the context of the dependent variable of interest. In this example, we are interested in blood cholesterol. However, `Blood mass concentation of total cholesterol` is a continuous variable. We shall convert this variable into a binary variable with two groups, Normal/Low and High cholesterol levels, by applying a [threshold of 200mg/dL](https://www.mayoclinic.org/diseases-conditions/high-blood-cholesterol/diagnosis-treatment/drc-20350806). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    list(facts[cholesterol_path] <= 200),\n",
    "    list(facts[cholesterol_path] > 200)\n",
    "]\n",
    "outputs = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.select(conditions, outputs)\n",
    "facts['categorical_cholesterol'] = pd.Series(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also specify the variable name for the covariate we are interested in, in this case Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_path = list(facts.filter(regex = 'sex'))[0]\n",
    "sex_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also select our cohorts of interest. In this example, we are interested in participants from the Framingham Heart Study (FHS) and the Atherosclerosis Risk In Communities (ARIC) cohort. We can utilize the `A distinct subgroup within a study  generally indicating subjects who share similar characteristics due to study design. Subjects may belong to only one subcohort.` concept path in the DCC Harmonized data set to select the participants of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_varnames = my_harmonized_dic[my_harmonized_dic.categorical == True]\n",
    "categorical_varnames = list(categorical_varnames.index)\n",
    "\n",
    "continuous_varnames = my_harmonized_dic[my_harmonized_dic.categorical == False]\n",
    "continuous_varnames = list(continuous_varnames.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts['FHS_cohort'] = facts.iloc[:,1].str.contains('FHS')\n",
    "facts['ARIC_cohort'] = facts.iloc[:,1].str.contains('ARIC')\n",
    "\n",
    "fhs_subset = facts[facts.FHS_cohort == True]\n",
    "aric_subset = facts[facts.ARIC_cohort == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Statistical analysis\n",
    "Two different association tests will be carried out according to variables data types:\n",
    "- Logistic regression for continuous variables, using the `Logit` statsmodels function\n",
    "- Fisher exact test for categorical variables, using the `fisher.test` R function\n",
    "\n",
    "We will create two functions, `test_continuous` and `test_categorical`, to perform these statistical tests. \n",
    "An additional function, `check_vars`, will be used to check if the data passes some assumptions of these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "#from sklearn import metrics\n",
    "#import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = fhs_subset[['categorical_cholesterol', continuous_varnames[1]]]\n",
    "dependent_vec = test_data['categorical_cholesterol']\n",
    "independent_vec = test_data[continuous_varnames[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(dependent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sm.Logit(dependent_vec, independent_vec, missing='drop')\n",
    "model_fit = model.fit().pvalues[0]\n",
    "model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_continuous(dependent_vec, independent_vec):\n",
    "    model = sm.Logit(dependent_vec, independent_vec, missing='drop')\n",
    "    pval = model.fit().pvalues[0]\n",
    "    return(pval)\n",
    "\n",
    "\n",
    "def check_vars(dependent_var, other_var, df, case_value, control_value):\n",
    "    check_pass = False\n",
    "    case = df[df[1] == case_value]\n",
    "    control = df[df[dependent_var == control_value]]\n",
    "    return case, control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1, test2 = check_vars('categorical_cholesterol', continuous_varnames[0], \n",
    "                         fhs_subset[['categorical_cholesterol', continuous_varnames[0]]], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phewas(facts, dependent_var, continuous_varnames, categorical_varnames, case_value, control_value):\n",
    "    results_df = pd.DataFrame(columns=['concept_code', \n",
    "                                       'simplified_varname', \n",
    "                                       'vartype',\n",
    "                                       'pval',\n",
    "                                       'n_cases',\n",
    "                                       'n_controls',\n",
    "                                       'var_cases',\n",
    "                                       'var_controls'\n",
    "                                      ])\n",
    "    \n",
    "    for other_var in continuous_varnames:\n",
    "        df = facts[[dependent_var, other_var]]\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run_phewas <- function(facts, dependent_var, continuous_varnames, categorical_varnames, case_value, control_value){\n",
    "\n",
    "    results_df <- data.frame('concept_code' = '', \n",
    "                             'simplified_varname' = '', \n",
    "                             'vartype' = '', \n",
    "                             'pval' = 1.0,\n",
    "                             'n_cases' = 0,\n",
    "                             'n_controls' = 0,\n",
    "                             'var_cases' = 0,\n",
    "                             'var_controls' = 0\n",
    "                            )[-1,]\n",
    "\n",
    "    for (other_var in continuous_varnames){\n",
    "        \n",
    "        df <- facts %>% select(eval(dependent_var), eval(other_var)) # do any filtering, removal of NA or blank values here if necessary (currently not doing so)\n",
    "        check <- check_vars(dependent_var, other_var, df, case_value, control_value)\n",
    "        if(check){\n",
    "        results_df <- results_df %>% add_row(concept_code = other_var, \n",
    "                                             simplified_varname = str_extract(other_var, '\\\\\\\\[^\\\\\\\\]*\\\\\\\\$') %>% str_replace_all('\\\\\\\\', ''), \n",
    "                                             vartype = 'continuous', \n",
    "                                             pval = test_continuous(df[,1], df[,2]),\n",
    "                                             n_cases = df %>% filter(get(dependent_var) == case_value, !is.na(get(other_var))) %>% nrow(),\n",
    "                                             n_controls = df %>% filter(get(dependent_var) == control_value, !is.na(get(other_var))) %>% nrow(),\n",
    "                                             var_cases = df %>% filter(get(dependent_var) == case_value) %>% pull(eval(other_var)) %>% var(na.rm = TRUE), #Variance\n",
    "                                             var_controls = df %>% filter(get(dependent_var) == control_value) %>% pull(eval(other_var)) %>% var(na.rm = TRUE)\n",
    "                                            )\n",
    "            }\n",
    "    }\n",
    "    for (other_var in categorical_varnames){\n",
    "        \n",
    "        df <- facts %>% select(eval(dependent_var), eval(other_var)) # do any filtering, removal of NA or blank values here if necessary (currently not doing so)\n",
    "        check <- check_vars(dependent_var, other_var, df, case_value, control_value)\n",
    "        if(check) {\n",
    "        results_df <- results_df %>% add_row(concept_code = other_var, \n",
    "                                             simplified_varname = str_extract(other_var, '\\\\\\\\[^\\\\\\\\]*\\\\\\\\$') %>% str_replace_all('\\\\\\\\', ''), \n",
    "                                             vartype = 'categorical', \n",
    "                                             pval = test_categorical(df[,2], df[,1]),\n",
    "                                             n_cases = df %>% filter(get(dependent_var) == case_value, !is.na(get(other_var))) %>% nrow(),\n",
    "                                             n_controls = df %>% filter(get(dependent_var) == control_value, !is.na(get(other_var))) %>% nrow(),\n",
    "                                             var_cases = NA,\n",
    "                                             var_controls = NA\n",
    "                                            )\n",
    "        \n",
    "            }\n",
    "        }\n",
    "    return(results_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "test_continuous <- function(dependent_vec, independent_vec){\n",
    "    \n",
    "    pval <- 999\n",
    "    glm.fit <- glm(dependent_vec ~ independent_vec, family = 'binomial')\n",
    "    try(pval <- coef(summary(glm.fit))['independent_vec','Pr(>|z|)'], silent = TRUE)\n",
    "    if (pval == 999) {pval <- NA}\n",
    "    \n",
    "    return(pval)\n",
    "}\n",
    "\n",
    "test_categorical <- function(dependent_vec, independent_vec) {\n",
    "    \n",
    "    contingency_table <- table(dependent_vec, independent_vec)\n",
    "    \n",
    "    # set arbitrary cutoff (5): do not compute stats for categorical variables with too many levels\n",
    "    if (max(dim(contingency_table)) > 5 | min(dim(contingency_table)) == 1) {\n",
    "        return(NA)\n",
    "    } else {\n",
    "        res <- fisher.test(contingency_table, workspace = 2e8, simulate.p.value=TRUE)\n",
    "        pval <- res$p.value\n",
    "        return(pval)\n",
    "    }   \n",
    "}\n",
    "\n",
    "check_vars <- function(dependent_var, other_var, df, case_value, control_value){\n",
    "    pass <- FALSE\n",
    "    cases <- df %>% filter(get(dependent_var) == case_value)\n",
    "    controls <- df %>% filter(get(dependent_var) == control_value)\n",
    "    \n",
    "    \n",
    "    try(if ((var(cases, na.rm = TRUE) != 0) & (var(controls, na.rm = TRUE) != 0)) {pass <- TRUE}, silent = TRUE)\n",
    "    \n",
    "    concept_vec <- df %>% pull(eval(other_var))\n",
    "    if(length(table(concept_vec)) > 1) {pass <- TRUE}\n",
    "    \n",
    "    return(pass)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "independent_names = variablesDict[\"name\"].tolist()\n",
    "independent_names.remove(dependent_var_name)\n",
    "dependent_var = facts[dependent_var_name].astype(\"category\").cat.codes\n",
    "dic_pvalues = {}\n",
    "simple_index_variablesDict = variablesDict.set_index(\"name\", drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import LinAlgError\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for independent_name in tqdm(independent_names, position=0, leave=True):\n",
    "    matrix = facts.loc[:, [dependent_var_name, independent_name]]\\\n",
    "                  .dropna(how=\"any\")\n",
    "    if matrix.shape[0] == 0:\n",
    "        dic_pvalues[independent_name] = np.NaN\n",
    "        continue\n",
    "    if simple_index_variablesDict.loc[independent_name, \"categorical\"]:\n",
    "        matrix = pd.get_dummies(matrix,\n",
    "                                columns=[independent_name],\n",
    "                                drop_first=False)\\\n",
    "                    .iloc[:, 0:-1]\n",
    "    dependent_var = matrix[dependent_var_name].cat.codes\n",
    "    independent_var = matrix.drop(dependent_var_name, axis=1)\\\n",
    "                            .assign(intercept = 1)\n",
    "    model = Logit(dependent_var, independent_var)\n",
    "    try:\n",
    "        results = model.fit(disp=0)\n",
    "        dic_pvalues[independent_name] = results.llr_pvalue\n",
    "    except (LinAlgError, PerfectSeparationError) as e:\n",
    "        dic_pvalues[independent_name] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin simran edits 01/05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are running many statistical tests, we need to perform a p-value adjustment. Here, we use the holm-bonferroni method with an alpha of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df$adj_pvalues <- p.adjust(combined_df$pval, method=\"holm\")\n",
    "### read in test data (generated from R phewas) for plotting\n",
    "plot_df = pd.read_csv('../R/phewas_df_LOCAL.csv')\n",
    "\n",
    "plot_df['neglogp'] = -np.log10(plot_df.pval); plot_df = plot_df.sort_values(['category'])\n",
    "plot_df.reset_index(inplace=True, drop=True); plot_df['i'] = plot_df.index\n",
    "\n",
    "\n",
    "adjusted_alpha = -np.log10(0.01)\n",
    "adjusted_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idenfity top 5 adjusted p value results for each study\n",
    "# ARIC\n",
    "aric_labels_df = plot_df[plot_df.study == 'ARIC']\n",
    "aric_labels_df = aric_labels_df[aric_labels_df.neglogp >= aric_labels_df.neglogp.sort_values(na_position = 'first').iloc[-5]]\n",
    "aric_labels_df['offset'] = [15, -15, -10, -10, -10]\n",
    "\n",
    "# FHS\n",
    "fhs_labels_df = plot_df[plot_df.study == 'FHS']\n",
    "fhs_labels_df = fhs_labels_df[fhs_labels_df.neglogp >= fhs_labels_df.neglogp.sort_values(na_position = 'first').iloc[-5]]\n",
    "fhs_labels_df['offset'] = [15, -15, -10, -10, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualization of results in a Manhattan plot\n",
    "We plot a Manhattan plot, commonly used in PheWAS analyses, to visualize our results. First we will organize our data for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and record categories for each concept code\n",
    "\n",
    "def categorize_function(x):\n",
    "    return(x.split('\\\\')[2])\n",
    "plot_df['category'] = plot_df['concept_code'].apply(categorize_function)\n",
    "plot_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x axis represents each of the phenotypes tested, and the y axis represents their associated -log10 p value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Manhattan plot for ARIC: \n",
    "# https://stackoverflow.com/questions/37463184/how-to-create-a-manhattan-plot-with-matplotlib-in-python\n",
    "\n",
    "plot = sns.relplot(data=plot_df[plot_df.study == 'ARIC'], \n",
    "                   x='i', y='neglogp', aspect=3.7, style = 'sex',\n",
    "                   hue='category', palette = 'bright') \n",
    "groups=plot_df.groupby('category')['i'].median()\n",
    "plot.ax.set_xlabel('Phenotype Category')\n",
    "plot.ax.set_ylabel('-log10(p-value)')\n",
    "#plot.ax.set_xticklabels(groups.index)\n",
    "plot.ax.set_xticklabels('')\n",
    "#plt.xticks(rotation = 10)\n",
    "for x, y, z, offset in zip(aric_labels_df['i'], aric_labels_df['neglogp'], aric_labels_df['simplified_varname'], aric_labels_df['offset']):\n",
    "    plt.text(x = x + 5, # x-coordinate position of data label\n",
    "             y = y + offset, # y-coordinate position of data label\n",
    "             s = z) \n",
    "plt.axhline(y=adjusted_alpha, color='r', linestyle = 'dotted')\n",
    "plot.fig.suptitle('Association between phenotypic variables and cholesterol level status within the Atherosclerosis Risk In Communities (ARIC) cohort');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Manhattan plot for FHS: \n",
    "# https://stackoverflow.com/questions/37463184/how-to-create-a-manhattan-plot-with-matplotlib-in-python\n",
    "\n",
    "plot = sns.relplot(data=plot_df[plot_df.study == 'FHS'], \n",
    "                   x='i', y='neglogp', aspect=3.7, style = 'sex',\n",
    "                   hue='category', palette = 'bright')\n",
    "groups=plot_df.groupby('category')['i'].median()\n",
    "plot.ax.set_xlabel('Phenotype Category')\n",
    "plot.ax.set_ylabel('-log10(p-value)')\n",
    "plot.ax.set_xticks(groups)\n",
    "#plot.ax.set_xticklabels(groups.index)\n",
    "plot.ax.set_xticklabels('')\n",
    "#plt.xticks(rotation = 10)\n",
    "# label points on the plot\n",
    "for x, y, z, offset in zip(fhs_labels_df['i'], fhs_labels_df['neglogp'], fhs_labels_df['simplified_varname'], fhs_labels_df['offset']):\n",
    "    plt.text(x = x + 5, # x-coordinate position of data label\n",
    "             y = y + offset, # y-coordinate position of data label\n",
    "             s = z)\n",
    "             #s = textwrap.fill(z, width=40, fix_sentence_endings=True, break_long_words=False)) #option for text wrapping, import textwrap\n",
    "plt.axhline(y=adjusted_alpha, color='r', linestyle = 'dotted')\n",
    "plot.fig.suptitle('Association between phenotypic variables and cholesterol level status within the Framingham Heart Study (FHS) cohort');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
